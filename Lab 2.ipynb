{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab 2**\n",
    "Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First We will summarize each of the following Libaries.**\n",
    "Libary one:\n",
    "1) ***NLTK:***\n",
    "This Platform Main goal is to make a bridge between Python programs and human languages data such as documents. The platform works with over 50 corporas(large strucuted sets of texts which are used for NLP works) and lexical resources.\n",
    "It's main abiliites that are relevent for our Dicts : Tokenization,Lemmatization,Stemming,Named Entity Recognition (NER),Synonyms(using wordnet) and WSD.\n",
    "2) ***TextBlob***\n",
    "This Libary is for processing textual data,The idea is to provide a simple API for working on NLPs tasks.\n",
    "It's main abiliites that are relevent for our Dicts :Tokenizations,lemmatizations,Spelling corrections.\n",
    "2) ***spaCy***\n",
    "Open-source libary,designed for producion use and helps to build apps \"understand\" large volumes of text.\n",
    "It's main abiliites that are relevent for our Dicts:Tokenization,Lemmatization,\n",
    "WSD(limited),NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Install all the libaries first.\n",
    "# %pip  install nltk\n",
    "# %pip install numpy\n",
    "# %pip install TextBlob\n",
    "# %pip install spaCy\n",
    "# %pip install pandas\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK is installed. Version: 3.9.1\n",
      "TextBlob is installed.\n",
      "SpaCy is installed. Version: 3.8.3\n"
     ]
    }
   ],
   "source": [
    "#Testing that the libaries are installed\n",
    "try:\n",
    "    import nltk\n",
    "    print(f\"NLTK is installed. Version: {nltk.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"NLTK is not installed.\")\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    print(\"TextBlob is installed.\")\n",
    "except ImportError:\n",
    "    print(\"TextBlob is not installed.\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    print(f\"SpaCy is installed. Version: {spacy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"SpaCy is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will test the libaries and their abillites on arteficual text.(60 words)\n",
    "Art_text = \"Jhon visited the bank to depsoit $500. He noticed the river bnak nearby was bustling with activty. AI is tranforming industres, especially helthcare and finance. NLP tols help anaylze txt. NASA recently annonced a mission to Mars, sparking global excitment. The CEO of Gogle praised inovation during his keynott speach.\"\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Text in tokenize form ['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '$', '500', '.', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'was', 'bustling', 'with', 'activty', '.', 'AI', 'is', 'tranforming', 'industres', ',', 'especially', 'helthcare', 'and', 'finance', '.', 'NLP', 'tols', 'help', 'anaylze', 'txt', '.', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', ',', 'sparking', 'global', 'excitment', '.', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach', '.']\n",
      "The lemmatized words {15: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 16: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 17: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 18: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 19: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 20: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 21: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 22: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 23: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 24: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 25: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 26: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 27: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 28: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 29: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 30: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 31: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 32: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 33: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 34: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 35: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 36: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 37: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 38: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 39: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 40: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 41: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 42: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 43: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 44: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 45: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 46: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 47: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 48: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 49: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 50: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 51: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 52: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 53: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 54: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 55: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 56: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 57: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 58: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}}\n"
     ]
    }
   ],
   "source": [
    "#Lets start With NLTK:\n",
    "#Abillites that it can run:Tokenization,Lemmatization,Stemming,Named Entity \n",
    "# Recognition (NER),Synonyms(using wordnet) and WSD.\n",
    "\n",
    "import nltk as nd\n",
    "import pandas as pd\n",
    "#Tokenize\n",
    "\n",
    "Art_text_tokenize =  nd.word_tokenize(Art_text)\n",
    "print(\"The Text in tokenize form\",Art_text_tokenize)\n",
    "\n",
    "#Lemmatization- will print only the ones that it changed.\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "word_after={}\n",
    "word_before={}\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(Art_text_tokenize):\n",
    "    word_before[i] = Art_text_tokenize[i]\n",
    "    word_after[i] = wnl.lemmatize(Art_text_tokenize[i])\n",
    "    if(word_after != word_before):\n",
    "        words_lemmatized[i] = word_after\n",
    "\n",
    "print(\"The lemmatized words\",words_lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that have been stemmed {0: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 1: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 2: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 3: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 4: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 5: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 6: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 7: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 8: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 9: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 10: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 11: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 12: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 13: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 14: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 15: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 16: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 17: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 18: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 19: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 20: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 21: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 22: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 23: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 24: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 25: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 26: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 27: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 28: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 29: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 30: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 31: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 32: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 33: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 34: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 35: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 36: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 37: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 38: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 39: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 40: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 41: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 42: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 43: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 44: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 45: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 46: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 47: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 48: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 49: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 50: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 51: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 52: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 53: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 54: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 55: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 56: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 57: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 58: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}}\n",
      "Words that have before being stemmed {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'was', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "#we will take again the same Art_text_tokenize\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "word_after={}\n",
    "word_before={}\n",
    "word_before_stemed = {}\n",
    "for i,word in enumerate(Art_text_tokenize):\n",
    "    word_before[i] = Art_text_tokenize[i]\n",
    "    word_after[i] = stemmer.stem(Art_text_tokenize[i])\n",
    "    if word_after != word_before:\n",
    "        word_before_stemed[i] = word_before[i]\n",
    "        word_stemed[i] = word_after\n",
    "print(f\"Words that have been stemmed\",word_stemed)\n",
    "print(f\"Words that have before being stemmed\",word_before_stemed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Installations for NER\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entites [('Jhon', 'PERSON'), ('NLP', 'ORGANIZATION'), ('NASA', 'ORGANIZATION'), ('Mars', 'PERSON'), ('CEO of Gogle', 'ORGANIZATION')]\n",
      "Words with at least one synonym and their synsets:\n",
      "ceo: chief_executive_officer.n.01, he: helium.n.01, ai: army_intelligence.n.01, visited: visit.v.01, praised: praise.v.01, finance: finance.n.01, mars: mars.n.01, nearby: nearby.s.01, noticed: detect.v.01, is: be.v.01, especially: particularly.r.01, mission: mission.n.01, was: washington.n.02, bank: bank.n.01, nasa: national_aeronautics_and_space_administration.n.01, sparking: trip.v.04, river: river.n.01, 500: five_hundred.n.01, help: aid.n.02, nlp: natural_language_processing.n.01, bustling: bustle.v.01, recently: recently.r.01, a: angstrom.n.01, global: global.s.01\n",
      "\n",
      "Words with their disambiguated senses and definitions:\n",
      "visited: visit.v.04 - come to see in an official or professional capacity\n",
      "bank: bank.v.07 - cover with ashes so to control the rate of burning\n",
      "500: five_hundred.s.01 - denoting a quantity consisting of 500 items or units\n",
      "He: helium.n.01 - a very light colorless element that is one of the six inert gasses; the most difficult gas to liquefy; occurs in economically extractable amounts in certain natural gases (as those found in Texas and Kansas)\n",
      "noticed: notice.v.04 - express recognition of the presence or existence of, or acquaintance with\n",
      "river: river.n.01 - a large natural stream of water (larger than a creek)\n",
      "nearby: nearby.s.01 - close at hand\n",
      "was: be.v.01 - have the quality of being; (copula, used with an adjective or a predicate noun)\n",
      "bustling: bustling.s.01 - full of energetic and noisy activity\n",
      "AI: three-toed_sloth.n.01 - a sloth that has three long claws on each forefoot and each hindfoot\n",
      "is: exist.v.01 - have an existence, be extant\n",
      "especially: particularly.r.01 - to a distinctly greater extent or degree than is common\n",
      "finance: finance.n.03 - the management of money and credit and banking and investments\n",
      "NLP: natural_language_processing.n.01 - the branch of information science that deals with natural language information\n",
      "help: serve.v.05 - help to some food; help with food or drink\n",
      "NASA: national_aeronautics_and_space_administration.n.01 - an independent agency of the United States government responsible for aviation and spaceflight\n",
      "recently: recently.r.01 - in the recent past\n",
      "a: angstrom.n.01 - a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation\n",
      "mission: mission.n.03 - a special assignment that is given to a person or group\n",
      "Mars: mars.n.01 - a small reddish planet that is the 4th from the sun and is periodically visible to the naked eye; minerals rich in iron cover its surface and are responsible for its characteristic color\n",
      "sparking: trip.v.04 - put in motion or move to act\n",
      "global: ball-shaped.s.01 - having the shape of a sphere or ball; ; ; - Zane Grey\n",
      "CEO: chief_executive_officer.n.01 - the corporate executive responsible for the operations of the firm; reports to a board of directors; may appoint other managers (including a president)\n",
      "praised: praise.v.01 - express approval of\n"
     ]
    }
   ],
   "source": [
    "#NLTK Named Entity \n",
    "# Recognition (NER)\n",
    "#We will use POS_Tags So the NER modal could work better.\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "pos_tags = pos_tag(Art_text_tokenize)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "print(\"Entites\",entites) \n",
    "\n",
    "#Synonyms\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = []\n",
    "\n",
    "for word in Art_text_tokenize:\n",
    "    word = word.lower()\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if wordnet.synsets(word):\n",
    "        if(word != synsets):\n",
    "            words_with_synonyms.append((word, synsets[0].name()))\n",
    "words_with_synonyms = list(set(words_with_synonyms))\n",
    "print(\"Words with at least one synonym and their synsets:\")\n",
    "formatted_output = ', '.join([f\"{word}: {synset}\" for word, synset in words_with_synonyms])\n",
    "print(formatted_output)\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "#we will try to use sentances to get context.\n",
    "sentences = nltk.sent_tokenize(Art_text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "\n",
    "# Display the results\n",
    "print(\"\")\n",
    "print(\"Words with their disambiguated senses and definitions:\")\n",
    "for word, synset_name, definition in word_senses:\n",
    "    print(f\"{word}: {synset_name} - {definition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization : ['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '500', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'was', 'bustling', 'with', 'activty', 'AI', 'is', 'tranforming', 'industres', 'especially', 'helthcare', 'and', 'finance', 'NLP', 'tols', 'help', 'anaylze', 'txt', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', 'sparking', 'global', 'excitment', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach']\n",
      "['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '500', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'wa', 'bustling', 'with', 'activty', 'AI', 'is', 'tranforming', 'industres', 'especially', 'helthcare', 'and', 'finance', 'NLP', 'tols', 'help', 'anaylze', 'txt', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', 'sparking', 'global', 'excitment', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach']\n",
      "word before : {'Jhon'} after: {'Hon'}\n",
      "word before : {'depsoit'} after: {'deposit'}\n",
      "word before : {'bnak'} after: {'bank'}\n",
      "word before : {'activty'} after: {'activity'}\n",
      "word before : {'AI'} after: {'of'}\n",
      "word before : {'tranforming'} after: {'transforming'}\n",
      "word before : {'industres'} after: {'industries'}\n",
      "word before : {'tols'} after: {'told'}\n",
      "word before : {'anaylze'} after: {'analyze'}\n",
      "word before : {'annonced'} after: {'announced'}\n",
      "word before : {'Mars'} after: {'Wars'}\n",
      "word before : {'sparking'} after: {'sparkling'}\n",
      "word before : {'excitment'} after: {'excitement'}\n",
      "word before : {'Gogle'} after: {'Sole'}\n",
      "word before : {'inovation'} after: {'innovation'}\n",
      "word before : {'speach'} after: {'speech'}\n"
     ]
    }
   ],
   "source": [
    "#Lets move to TextBlob\n",
    "#Tokenizations,lemmatizations,Spelling corrections.\n",
    "from textblob import TextBlob \n",
    "#Tokenization\n",
    "blob_obj = TextBlob(Art_text)\n",
    "blob_tokenize = blob_obj.words\n",
    "print(\"Tokenization :\",blob_tokenize)\n",
    "\n",
    "#Lemmatizations\n",
    "blob_lem=[word.lemmatize() for word in blob_tokenize]\n",
    "print(blob_lem)\n",
    "\n",
    "#Spelling Corrections\n",
    "for i,word in enumerate(blob_tokenize):\n",
    "    word_before = blob_tokenize[i]\n",
    "    word_after =blob_tokenize[i].correct()\n",
    "    if word_before != word_after:\n",
    "        print(f\"word before :\",{word_before},\"after:\",{word_after})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art_token ['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '$', '500', '.', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'was', 'bustling', 'with', 'activty', '.', 'AI', 'is', 'tranforming', 'industres', ',', 'especially', 'helthcare', 'and', 'finance', '.', 'NLP', 'tols', 'help', 'anaylze', 'txt', '.', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', ',', 'sparking', 'global', 'excitment', '.', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach', '.']\n",
      "Lemmatization ['Jhon', 'visit', 'the', 'bank', 'to', 'depsoit', '$', '500', '.', 'he', 'notice', 'the', 'river', 'bnak', 'nearby', 'be', 'bustle', 'with', 'activty', '.', 'AI', 'be', 'tranforme', 'industre', ',', 'especially', 'helthcare', 'and', 'finance', '.', 'NLP', 'tol', 'help', 'anaylze', 'txt', '.', 'NASA', 'recently', 'annonce', 'a', 'mission', 'to', 'Mars', ',', 'spark', 'global', 'excitment', '.', 'the', 'ceo', 'of', 'Gogle', 'praise', 'inovation', 'during', 'his', 'keynott', 'speach', '.']\n",
      "Entity: Jhon, Label: PERSON\n",
      "Entity: depsoit, Label: GPE\n",
      "Entity: 500, Label: MONEY\n",
      "Entity: NLP, Label: ORG\n",
      "Entity: NASA, Label: ORG\n",
      "Entity: Mars, Label: LOC\n",
      "Entity: Gogle, Label: ORG\n",
      "Entity: keynott speach, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "#SPACY\n",
    "# Tokenization,Lemmatization, WSD(limited),NER.\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "nlp = English()#english pipline\n",
    "\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(Art_text)\n",
    "Art_token = [token.text for token in tokens]\n",
    "print(\"Art_token\",Art_token)\n",
    "\n",
    "#Lemmatization\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(Art_text) #we need a raw version.\n",
    "print(\"Lemmatization\",[token.lemma_ for token in doc])\n",
    "\n",
    "#NER\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**part 4**\n",
    "Working on our choosen docs for lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') #punctuations\n",
    "nltk.download('stopwords')#Stopwords.\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "cats = ['comp.windows.x', 'sci.space'] #getting two options\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize #Libray to tokenize the words.\n",
    "from nltk.corpus import stopwords\n",
    "import string  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  #Libary from Counters.\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=cats)\n",
    "\n",
    "\n",
    "#lets do some preprocessing\n",
    "#First lets combine them into one big category.\n",
    "category_texts_unified = [\n",
    "    {\"text\": doc, \"category\": newsgroups.target_names[target]}\n",
    "    for doc, target in zip(newsgroups.data, newsgroups.target)\n",
    "    if target in [newsgroups.target_names.index(cat) for cat in cats]\n",
    "]\n",
    "stops = set(stopwords.words('english')) # stopword from english.\n",
    "\n",
    "#Removing Lowercases,non-alphabetical characters\n",
    "#Tokenizeing ,removing stopwords and punctuation\n",
    "Clean_text = []\n",
    "for doc in category_texts_unified:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', doc[\"text\"].lower())\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [w for w in words if w not in stops and w not in string.punctuation]\n",
    "    # Save cleaned text with category\n",
    "    Clean_text.append({\"text\": ' '.join(filtered_words), \"category\": doc[\"category\"]})\n",
    "\n",
    "\n",
    "\n",
    "#Let's look for mispalling.\n",
    "mispalled_words = []\n",
    "for entry in Clean_text:\n",
    "    # Tokenize the cleaned text\n",
    "    words = word_tokenize(entry[\"text\"])\n",
    "    for word in words:\n",
    "        blob = TextBlob(word)\n",
    "        corrected_word = str(blob.correct())\n",
    "        if corrected_word != word:\n",
    "            mispalled_words.append((word, corrected_word))\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # Save preprocessed data to a Pickle file\n",
    "# with open(\"preprocessed_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(Clean_text, f)\n",
    "\n",
    "# print(\"Data saved to preprocessed_data.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after preprocssing, We will start doing the abilities we mentioned in the begining.\n",
    "We can conclude that fixing mispalling takes the most time and effort, therefore as part of our attempts to find the best algorthem for both accurcy and time efficency.\n",
    "We will judge the outputs on how many ACRONYM,SYNONYMS,WSD,NER;we got on each test.\n",
    "#For the first test, We will do :\n",
    "1)lemmatization-nltk \n",
    "2)stemming -nltk\n",
    "3)Synonyms -nltk using wordnet\n",
    "4)WSD-nltk\n",
    "5)NER-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 1.8750958442687988\n",
      "lemmatization time: 1.8051435947418213\n",
      "Stemming time: 4.695420265197754\n",
      "Synonyms time: 7.5493669509887695\n",
      "WSD time: 9809.51560473442\n",
      "NER time: 91.48276424407959\n",
      "The Results are in!\n",
      "The amount of lemmatized words: 33513\n",
      "The amount of stemmed words: 134885\n",
      "The amount of words with at least two synonyms: 10405\n",
      "The amount of NER entities found: 3\n",
      "The amount of words with sense found (WSD): 257634\n",
      "Overall time it took: 9916.932371377945\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import nltk as nd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    " \n",
    "start_time_overall = time.time() #Starting the timer.\n",
    "#Tokenization\n",
    "start_time_task = time.time()\n",
    "data_tokenized = []\n",
    "for entry in data:\n",
    "    tokenized_data = nd.word_tokenize(entry[\"text\"])\n",
    "    data_tokenized.extend(tokenized_data)  # Flatten tokens into a single list\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Tokenization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#lemmatization\n",
    "start_time_task = time.time()\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    word_afte = wnl.lemmatize(word)\n",
    "    if(word_afte != word):\n",
    "        words_lemmatized[i] = (word,word_afte)\n",
    "end_time_task = time.time()\n",
    "print(\"lemmatization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#stemming\n",
    "start_time_task = time.time()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    stemed_word = stemmer.stem(word)\n",
    "    if stemed_word != word:\n",
    "        word_stemed[i] = (word,stemed_word)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Stemming time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "#Synonyms-we will count the amout of words that have at least two synonyms\n",
    "start_time_task = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = {}\n",
    "\n",
    "for word in data_tokenized:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets)>=2: #checks if there is at least two synonmys per word.\n",
    "        synonyms = {lemma.name() for syn in synsets for lemma in syn.lemmas()}\n",
    "        if len(synonyms) >= 2:  # Check if there are at least two distinct synonyms\n",
    "            words_with_synonyms[word] = list(synonyms)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Synonyms time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "#we will try to use sentances to get context.\n",
    "start_time_task = time.time()\n",
    "\n",
    "text_string = \" \".join(data_tokenized)\n",
    "sentences = nd.sent_tokenize(text_string)\n",
    "tokenized_sentences = [nd.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "end_time_task = time.time()\n",
    "print(\"WSD time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "\n",
    "#NER\n",
    "start_time_task = time.time()\n",
    "pos_tags = pos_tag(data_tokenized)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"NER time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"The Results are in!\")\n",
    "print(f'The amount of lemmatized words: {len(words_lemmatized)}')\n",
    "print(f'The amount of stemmed words: {len(word_stemed)}')\n",
    "print(f'The amount of words with at least two synonyms: {len(words_with_synonyms)}')\n",
    "print(f'The amount of NER entities found: {len(entites)}')\n",
    "print(f'The amount of words with sense found (WSD): {len(word_senses)}')\n",
    "print(\"Overall time it took:\",exc_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 1.815438985824585\n",
      "lemmatization time: 5.336212396621704\n",
      "Synonyms time: 10.8156259059906\n",
      "Stemming time: 5.025677919387817\n",
      "NER time: 94.18226456642151\n",
      "WSD time: 8215.4396276474\n",
      "The Results are in!\n",
      "The amount of lemmatized words: 33513\n",
      "The amount of stemmed words: 134885\n",
      "The amount of words with at least two synonyms: 10405\n",
      "The amount of NER entities found: 3\n",
      "The amount of words with sense found (WSD): 257634\n",
      "Overall time it took: 8332.61590385437\n"
     ]
    }
   ],
   "source": [
    "#lets try diffrenet sequence of abiliities.\n",
    "#1)lemmatization-nltk \n",
    "#2)Synonyms -nltk using wordnet\n",
    "#3)stemming -nltk\n",
    "#4)NER-nltk\n",
    "#5)WSD-nltk\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import nltk as nd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "start_time_overall = time.time() #Starting the timer.\n",
    "#Tokenization\n",
    "start_time_task = time.time()\n",
    "data_tokenized = []\n",
    "for entry in data:\n",
    "    tokenized_data = nd.word_tokenize(entry[\"text\"])\n",
    "    data_tokenized.extend(tokenized_data)  # Flatten tokens into a single list\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Tokenization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#lemmatization\n",
    "start_time_task = time.time()\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    word_afte = wnl.lemmatize(word)\n",
    "    if(word_afte != word):\n",
    "        words_lemmatized[i] = (word,word_afte)\n",
    "end_time_task = time.time()\n",
    "print(\"lemmatization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#Synonyms-we will count the amout of words that have at least two synonyms\n",
    "start_time_task = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = {}\n",
    "\n",
    "for word in data_tokenized:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets)>=2: #checks if there is at least two synonmys per word.\n",
    "        synonyms = {lemma.name() for syn in synsets for lemma in syn.lemmas()}\n",
    "        if len(synonyms) >= 2:  # Check if there are at least two distinct synonyms\n",
    "            words_with_synonyms[word] = list(synonyms)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Synonyms time:\",end_time_task-start_time_task)\n",
    "\n",
    "#stemming\n",
    "start_time_task = time.time()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    stemed_word = stemmer.stem(word)\n",
    "    if stemed_word != word:\n",
    "        word_stemed[i] = (word,stemed_word)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Stemming time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "#NER\n",
    "start_time_task = time.time()\n",
    "pos_tags = pos_tag(data_tokenized)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"NER time:\",end_time_task-start_time_task)\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "#we will try to use sentances to get context.\n",
    "start_time_task = time.time()\n",
    "\n",
    "text_string = \" \".join(data_tokenized)\n",
    "sentences = nd.sent_tokenize(text_string)\n",
    "tokenized_sentences = [nd.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "end_time_task = time.time()\n",
    "print(\"WSD time:\",end_time_task-start_time_task)\n",
    "\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"The Results are in!\")\n",
    "print(f'The amount of lemmatized words: {len(words_lemmatized)}')\n",
    "print(f'The amount of stemmed words: {len(word_stemed)}')\n",
    "print(f'The amount of words with at least two synonyms: {len(words_with_synonyms)}')\n",
    "print(f'The amount of NER entities found: {len(entites)}')\n",
    "print(f'The amount of words with sense found (WSD): {len(word_senses)}')\n",
    "print(\"Overall time it took:\",exc_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Test \n",
    "#1)WSD-nltk\n",
    "#2)NER-nltk\n",
    "#3)lemmatization-nltk \n",
    "#4)Synonyms -nltk using wordnet\n",
    "#5)stemming -nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 1.7612321376800537\n",
      "WSD time: 7260.057300329208\n",
      "NER time: 84.98901629447937\n",
      "lemmatization time: 1.7268247604370117\n",
      "Synonyms time: 7.27872109413147\n",
      "Stemming time: 4.529211521148682\n",
      "The Results are in!\n",
      "The amount of lemmatized words: 33513\n",
      "The amount of stemmed words: 134885\n",
      "The amount of words with at least two synonyms: 10405\n",
      "The amount of NER entities found: 3\n",
      "The amount of words with sense found (WSD): 257634\n",
      "Overall time it took: 7360.344216585159\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import nltk as nd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "start_time_overall = time.time() #Starting the timer.\n",
    "#Tokenization\n",
    "start_time_task = time.time()\n",
    "data_tokenized = []\n",
    "for entry in data:\n",
    "    tokenized_data = nd.word_tokenize(entry[\"text\"])\n",
    "    data_tokenized.extend(tokenized_data)  # Flatten tokens into a single list\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Tokenization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "#we will try to use sentances to get context.\n",
    "start_time_task = time.time()\n",
    "\n",
    "text_string = \" \".join(data_tokenized)\n",
    "sentences = nd.sent_tokenize(text_string)\n",
    "tokenized_sentences = [nd.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "end_time_task = time.time()\n",
    "print(\"WSD time:\",end_time_task-start_time_task)\n",
    "\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "#NER\n",
    "start_time_task = time.time()\n",
    "pos_tags = pos_tag(data_tokenized)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"NER time:\",end_time_task-start_time_task)\n",
    "\n",
    "#lemmatization\n",
    "start_time_task = time.time()\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    word_afte = wnl.lemmatize(word)\n",
    "    if(word_afte != word):\n",
    "        words_lemmatized[i] = (word,word_afte)\n",
    "end_time_task = time.time()\n",
    "print(\"lemmatization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#Synonyms-we will count the amout of words that have at least two synonyms\n",
    "start_time_task = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = {}\n",
    "\n",
    "for word in data_tokenized:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets)>=2: #checks if there is at least two synonmys per word.\n",
    "        synonyms = {lemma.name() for syn in synsets for lemma in syn.lemmas()}\n",
    "        if len(synonyms) >= 2:  # Check if there are at least two distinct synonyms\n",
    "            words_with_synonyms[word] = list(synonyms)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Synonyms time:\",end_time_task-start_time_task)\n",
    "\n",
    "#stemming\n",
    "start_time_task = time.time()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    stemed_word = stemmer.stem(word)\n",
    "    if stemed_word != word:\n",
    "        word_stemed[i] = (word,stemed_word)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Stemming time:\",end_time_task-start_time_task)\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "# Display the results\n",
    "print(\"The Results are in!\")\n",
    "print(f'The amount of lemmatized words: {len(words_lemmatized)}')\n",
    "print(f'The amount of stemmed words: {len(word_stemed)}')\n",
    "print(f'The amount of words with at least two synonyms: {len(words_with_synonyms)}')\n",
    "print(f'The amount of NER entities found: {len(entites)}')\n",
    "print(f'The amount of words with sense found (WSD): {len(word_senses)}')\n",
    "print(\"Overall time it took:\",exc_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that The best organzation of the abiliites is the last test , and all the other tests preformed the same, but the last test was the fastest.\n",
    "\n",
    "**lab 2 part 2**\n",
    "In this part we will first examine 100 docs, and then for 1000 docs.\n",
    "We will build in the end ,Matrix boolean and reversed indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Matrix of 100 :\n",
      " [[1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 1]\n",
      " [1 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "Count of '1's: 64\n",
      "It took: 0.008994340896606445\n"
     ]
    }
   ],
   "source": [
    "# #Lets start with taking 100 docs from our preprossced data.\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "\n",
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create a dictionary of 100 documents\n",
    "docs_100 = {f\"doc_{i+1}\": data[i] for i in range(min(100, len(data)))}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(doc):\n",
    "    text = doc.get('text', '')  \n",
    "    text = text.lower()  \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  \n",
    "    return text.split()  \n",
    "\n",
    "\n",
    "docs_100_tokenized = {doc: preprocess_and_tokenize(text) for doc, text in docs_100.items()}\n",
    "\n",
    "\n",
    "query_words = [\"space\", \"computers\", \"university\"]  # Convert query words to lowercase\n",
    "\n",
    "start_time_space = time.time()\n",
    "\n",
    "\n",
    "bool_matrix = np.array([[int(word in doc) for word in query_words] for doc in docs_100_tokenized.values()])\n",
    "\n",
    "# Count the total number of '1's (matches)\n",
    "count = np.sum(bool_matrix)\n",
    "\n",
    "# Print the results\n",
    "print(\"Boolean Matrix of 100 :\\n\", bool_matrix)\n",
    "print(\"Count of '1's:\", count)\n",
    "\n",
    "end_time_space = time.time()\n",
    "print(\"It took:\", end_time_space - start_time_space)\n",
    "\n",
    "# Save the Boolean matrix to a Pickle file\n",
    "with open(\"Bool_matrix_100_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bool_matrix, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Matrix of 1000 :\n",
      " [[1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " ...\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Count of '1's: 601\n",
      "It took: 0.030632734298706055\n"
     ]
    }
   ],
   "source": [
    "#Lets work on 1000 documents and show the boolean matrix of it.\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create a dictionary of 1000 documents\n",
    "docs_100 = {f\"doc_{i+1}\": data[i] for i in range(min(1000, len(data)))}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(doc):\n",
    "    text = doc.get('text', '')  \n",
    "    text = text.lower()  \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  \n",
    "    return text.split()  \n",
    "\n",
    "\n",
    "docs_100_tokenized = {doc: preprocess_and_tokenize(text) for doc, text in docs_100.items()}\n",
    "\n",
    "\n",
    "query_words = [\"space\", \"computers\", \"university\"]  # Convert query words to lowercase\n",
    "\n",
    "start_time_space = time.time()\n",
    "\n",
    "\n",
    "bool_matrix = np.array([[int(word in doc) for word in query_words] for doc in docs_100_tokenized.values()])\n",
    "\n",
    "# Count the total number of '1's (matches)\n",
    "count = np.sum(bool_matrix)\n",
    "\n",
    "# Print the results\n",
    "print(\"Boolean Matrix of 1000 :\\n\", bool_matrix)\n",
    "print(\"Count of '1's:\", count)\n",
    "\n",
    "end_time_space = time.time()\n",
    "print(\"It took:\", end_time_space - start_time_space)\n",
    "\n",
    "# Save the Boolean matrix to a Pickle file\n",
    "with open(\"Bool_matrix_1000_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bool_matrix, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will work on index reversing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index for Query Words:\n",
      "space: [('doc_1', 1), ('doc_2', 1), ('doc_4', 1), ('doc_6', 1), ('doc_12', 3), ('doc_13', 1), ('doc_16', 71), ('doc_18', 1), ('doc_19', 1), ('doc_30', 3), ('doc_31', 3), ('doc_36', 3), ('doc_40', 5), ('doc_42', 4), ('doc_43', 1), ('doc_47', 1), ('doc_49', 2), ('doc_51', 2), ('doc_53', 3), ('doc_58', 1), ('doc_62', 4), ('doc_63', 1), ('doc_69', 2), ('doc_77', 1), ('doc_79', 3), ('doc_82', 4), ('doc_87', 1), ('doc_88', 5), ('doc_89', 1), ('doc_91', 1), ('doc_97', 4), ('doc_99', 1), ('doc_100', 4)]\n",
      "computers: []\n",
      "university: [('doc_3', 1), ('doc_8', 1), ('doc_13', 1), ('doc_17', 1), ('doc_19', 1), ('doc_21', 2), ('doc_22', 1), ('doc_26', 1), ('doc_31', 1), ('doc_32', 1), ('doc_34', 1), ('doc_36', 1), ('doc_41', 1), ('doc_44', 1), ('doc_45', 2), ('doc_51', 1), ('doc_57', 1), ('doc_59', 1), ('doc_62', 1), ('doc_63', 1), ('doc_66', 1), ('doc_77', 3), ('doc_78', 1), ('doc_79', 1), ('doc_82', 1), ('doc_83', 1), ('doc_90', 1), ('doc_94', 1), ('doc_95', 1), ('doc_97', 1), ('doc_98', 1)]\n",
      "space: 33 documents\n",
      "computers: 0 documents\n",
      "university: 31 documents\n",
      "The amount it found {'space': [('doc_1', 1), ('doc_2', 1), ('doc_4', 1), ('doc_6', 1), ('doc_12', 3), ('doc_13', 1), ('doc_16', 71), ('doc_18', 1), ('doc_19', 1), ('doc_30', 3), ('doc_31', 3), ('doc_36', 3), ('doc_40', 5), ('doc_42', 4), ('doc_43', 1), ('doc_47', 1), ('doc_49', 2), ('doc_51', 2), ('doc_53', 3), ('doc_58', 1), ('doc_62', 4), ('doc_63', 1), ('doc_69', 2), ('doc_77', 1), ('doc_79', 3), ('doc_82', 4), ('doc_87', 1), ('doc_88', 5), ('doc_89', 1), ('doc_91', 1), ('doc_97', 4), ('doc_99', 1), ('doc_100', 4)], 'computers': [], 'university': [('doc_3', 1), ('doc_8', 1), ('doc_13', 1), ('doc_17', 1), ('doc_19', 1), ('doc_21', 2), ('doc_22', 1), ('doc_26', 1), ('doc_31', 1), ('doc_32', 1), ('doc_34', 1), ('doc_36', 1), ('doc_41', 1), ('doc_44', 1), ('doc_45', 2), ('doc_51', 1), ('doc_57', 1), ('doc_59', 1), ('doc_62', 1), ('doc_63', 1), ('doc_66', 1), ('doc_77', 3), ('doc_78', 1), ('doc_79', 1), ('doc_82', 1), ('doc_83', 1), ('doc_90', 1), ('doc_94', 1), ('doc_95', 1), ('doc_97', 1), ('doc_98', 1)]}\n",
      "It took: 0.007512807846069336\n"
     ]
    }
   ],
   "source": [
    "#Lets start with 100 docs.\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create a dictionary of 100 documents\n",
    "docs_100 = {f\"doc_{i+1}\": data[i] for i in range(min(100, len(data)))}\n",
    "\n",
    "def preprocess_and_tokenize(doc):\n",
    "    text = doc.get('text', '')  \n",
    "    text = text.lower()  \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  \n",
    "    return text.split()  \n",
    "\n",
    "\n",
    "docs_100_tokenized = {doc: preprocess_and_tokenize(text) for doc, text in docs_100.items()}\n",
    "\n",
    "\n",
    "query_words = [\"space\", \"computers\", \"university\"]  \n",
    "\n",
    "start_time_space = time.time()\n",
    "\n",
    "inverted_index_query_words = {word: [] for word in query_words}\n",
    "\n",
    "for doc_name, tokens in docs_100_tokenized.items():\n",
    "    for word in query_words:\n",
    "        word_count = tokens.count(word)  # Count occurrences of the query word\n",
    "        if word_count > 0:\n",
    "            inverted_index_query_words[word].append((doc_name, word_count))  \n",
    "\n",
    "print(\"Inverted Index for Query Words:\")\n",
    "for word, doc_list in inverted_index_query_words.items():\n",
    "    print(f\"{word}: {doc_list}\")\n",
    "\n",
    "\n",
    "word_doc_counts = {}\n",
    "for word, doc_list in inverted_index_query_words.items():\n",
    "    doc_count = len(doc_list)  # Count the number of documents containing the word\n",
    "    word_doc_counts[word] = doc_count  # Store the count for each word\n",
    "    print(f\"{word}: {doc_count} documents\")\n",
    "print(\"The amount it found\",count)\n",
    "\n",
    "end_time_space = time.time()\n",
    "print(\"It took:\", end_time_space - start_time_space)\n",
    "\n",
    "with open(\"index_inverted_100_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(inverted_index_query_words, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index for Query Words:\n",
      "space: [('doc_1', 1), ('doc_2', 1), ('doc_4', 1), ('doc_6', 1), ('doc_12', 3), ('doc_13', 1), ('doc_16', 71), ('doc_18', 1), ('doc_19', 1), ('doc_30', 3), ('doc_31', 3), ('doc_36', 3), ('doc_40', 5), ('doc_42', 4), ('doc_43', 1), ('doc_47', 1), ('doc_49', 2), ('doc_51', 2), ('doc_53', 3), ('doc_58', 1), ('doc_62', 4), ('doc_63', 1), ('doc_69', 2), ('doc_77', 1), ('doc_79', 3), ('doc_82', 4), ('doc_87', 1), ('doc_88', 5), ('doc_89', 1), ('doc_91', 1), ('doc_97', 4), ('doc_99', 1), ('doc_100', 4), ('doc_101', 3), ('doc_105', 1), ('doc_108', 1), ('doc_113', 1), ('doc_114', 2), ('doc_116', 4), ('doc_118', 3), ('doc_124', 2), ('doc_128', 1), ('doc_129', 1), ('doc_131', 1), ('doc_134', 2), ('doc_137', 2), ('doc_139', 2), ('doc_142', 1), ('doc_147', 1), ('doc_150', 1), ('doc_152', 1), ('doc_158', 2), ('doc_160', 4), ('doc_163', 3), ('doc_171', 5), ('doc_183', 2), ('doc_190', 2), ('doc_194', 2), ('doc_195', 4), ('doc_197', 2), ('doc_203', 3), ('doc_206', 1), ('doc_211', 2), ('doc_213', 1), ('doc_214', 6), ('doc_216', 2), ('doc_217', 4), ('doc_224', 3), ('doc_226', 2), ('doc_230', 1), ('doc_231', 14), ('doc_240', 1), ('doc_241', 2), ('doc_245', 3), ('doc_247', 6), ('doc_248', 3), ('doc_258', 6), ('doc_259', 2), ('doc_261', 1), ('doc_264', 3), ('doc_267', 1), ('doc_268', 2), ('doc_270', 1), ('doc_274', 8), ('doc_276', 1), ('doc_280', 2), ('doc_285', 2), ('doc_288', 3), ('doc_289', 1), ('doc_295', 2), ('doc_296', 1), ('doc_298', 2), ('doc_300', 1), ('doc_308', 1), ('doc_311', 5), ('doc_312', 1), ('doc_314', 1), ('doc_327', 1), ('doc_329', 1), ('doc_334', 2), ('doc_335', 7), ('doc_336', 4), ('doc_342', 1), ('doc_343', 3), ('doc_346', 9), ('doc_347', 2), ('doc_352', 2), ('doc_353', 10), ('doc_358', 1), ('doc_363', 1), ('doc_367', 1), ('doc_374', 5), ('doc_379', 6), ('doc_384', 27), ('doc_392', 5), ('doc_395', 2), ('doc_396', 2), ('doc_397', 2), ('doc_402', 29), ('doc_403', 3), ('doc_406', 2), ('doc_409', 2), ('doc_412', 1), ('doc_413', 1), ('doc_414', 2), ('doc_416', 1), ('doc_423', 2), ('doc_424', 2), ('doc_428', 2), ('doc_437', 1), ('doc_441', 1), ('doc_442', 3), ('doc_443', 2), ('doc_448', 3), ('doc_451', 1), ('doc_457', 1), ('doc_458', 5), ('doc_459', 2), ('doc_461', 5), ('doc_465', 1), ('doc_467', 1), ('doc_469', 1), ('doc_472', 1), ('doc_475', 1), ('doc_478', 1), ('doc_485', 4), ('doc_486', 4), ('doc_491', 2), ('doc_500', 1), ('doc_508', 1), ('doc_509', 1), ('doc_510', 2), ('doc_512', 2), ('doc_520', 1), ('doc_527', 1), ('doc_533', 1), ('doc_535', 1), ('doc_536', 1), ('doc_538', 3), ('doc_545', 6), ('doc_547', 2), ('doc_549', 1), ('doc_554', 3), ('doc_558', 1), ('doc_568', 3), ('doc_575', 3), ('doc_577', 3), ('doc_578', 5), ('doc_582', 1), ('doc_585', 1), ('doc_586', 1), ('doc_587', 2), ('doc_589', 1), ('doc_590', 2), ('doc_593', 1), ('doc_595', 1), ('doc_600', 3), ('doc_603', 1), ('doc_605', 2), ('doc_609', 5), ('doc_611', 1), ('doc_613', 1), ('doc_615', 2), ('doc_617', 1), ('doc_621', 1), ('doc_623', 2), ('doc_630', 1), ('doc_635', 37), ('doc_638', 2), ('doc_642', 4), ('doc_643', 7), ('doc_646', 2), ('doc_647', 3), ('doc_648', 36), ('doc_653', 4), ('doc_654', 6), ('doc_661', 2), ('doc_666', 10), ('doc_668', 1), ('doc_669', 5), ('doc_670', 1), ('doc_671', 2), ('doc_676', 1), ('doc_682', 3), ('doc_684', 4), ('doc_689', 10), ('doc_693', 2), ('doc_696', 1), ('doc_700', 1), ('doc_706', 2), ('doc_708', 3), ('doc_712', 10), ('doc_713', 2), ('doc_715', 1), ('doc_716', 1), ('doc_718', 19), ('doc_719', 1), ('doc_720', 2), ('doc_722', 8), ('doc_724', 1), ('doc_729', 3), ('doc_738', 3), ('doc_741', 1), ('doc_755', 6), ('doc_756', 1), ('doc_761', 5), ('doc_763', 5), ('doc_769', 1), ('doc_778', 1), ('doc_788', 3), ('doc_792', 7), ('doc_796', 4), ('doc_804', 3), ('doc_811', 2), ('doc_812', 5), ('doc_815', 3), ('doc_821', 2), ('doc_823', 1), ('doc_825', 1), ('doc_833', 1), ('doc_835', 1), ('doc_836', 9), ('doc_837', 1), ('doc_838', 3), ('doc_839', 3), ('doc_843', 32), ('doc_844', 3), ('doc_848', 1), ('doc_856', 1), ('doc_860', 1), ('doc_862', 1), ('doc_865', 2), ('doc_873', 4), ('doc_874', 2), ('doc_881', 13), ('doc_882', 2), ('doc_886', 1), ('doc_890', 35), ('doc_898', 4), ('doc_906', 5), ('doc_907', 2), ('doc_910', 4), ('doc_914', 2), ('doc_917', 1), ('doc_920', 1), ('doc_924', 1), ('doc_925', 11), ('doc_927', 4), ('doc_929', 1), ('doc_930', 2), ('doc_933', 2), ('doc_934', 3), ('doc_940', 1), ('doc_942', 1), ('doc_944', 2), ('doc_946', 2), ('doc_953', 1), ('doc_954', 10), ('doc_959', 3), ('doc_960', 2), ('doc_962', 4), ('doc_963', 1), ('doc_969', 2), ('doc_974', 3), ('doc_985', 3), ('doc_989', 3), ('doc_995', 1)]\n",
      "computers: [('doc_174', 3), ('doc_287', 1), ('doc_296', 1), ('doc_311', 1), ('doc_648', 2), ('doc_843', 1), ('doc_890', 5), ('doc_899', 1), ('doc_959', 1)]\n",
      "university: [('doc_3', 1), ('doc_8', 1), ('doc_13', 1), ('doc_17', 1), ('doc_19', 1), ('doc_21', 2), ('doc_22', 1), ('doc_26', 1), ('doc_31', 1), ('doc_32', 1), ('doc_34', 1), ('doc_36', 1), ('doc_41', 1), ('doc_44', 1), ('doc_45', 2), ('doc_51', 1), ('doc_57', 1), ('doc_59', 1), ('doc_62', 1), ('doc_63', 1), ('doc_66', 1), ('doc_77', 3), ('doc_78', 1), ('doc_79', 1), ('doc_82', 1), ('doc_83', 1), ('doc_90', 1), ('doc_94', 1), ('doc_95', 1), ('doc_97', 1), ('doc_98', 1), ('doc_103', 1), ('doc_105', 1), ('doc_107', 1), ('doc_108', 1), ('doc_112', 1), ('doc_113', 2), ('doc_117', 2), ('doc_118', 2), ('doc_124', 1), ('doc_127', 2), ('doc_131', 2), ('doc_134', 1), ('doc_135', 1), ('doc_139', 1), ('doc_147', 1), ('doc_157', 1), ('doc_159', 1), ('doc_160', 1), ('doc_161', 1), ('doc_164', 1), ('doc_168', 1), ('doc_188', 1), ('doc_190', 1), ('doc_196', 3), ('doc_203', 1), ('doc_212', 3), ('doc_213', 1), ('doc_214', 1), ('doc_216', 1), ('doc_218', 2), ('doc_222', 2), ('doc_224', 1), ('doc_225', 1), ('doc_229', 1), ('doc_231', 1), ('doc_237', 3), ('doc_239', 1), ('doc_240', 1), ('doc_241', 1), ('doc_242', 1), ('doc_247', 1), ('doc_253', 1), ('doc_254', 1), ('doc_255', 1), ('doc_259', 1), ('doc_260', 1), ('doc_261', 1), ('doc_274', 1), ('doc_283', 1), ('doc_294', 1), ('doc_295', 1), ('doc_296', 4), ('doc_298', 1), ('doc_300', 1), ('doc_304', 1), ('doc_307', 1), ('doc_309', 1), ('doc_314', 1), ('doc_318', 2), ('doc_322', 1), ('doc_329', 4), ('doc_330', 1), ('doc_331', 1), ('doc_332', 1), ('doc_333', 1), ('doc_334', 1), ('doc_335', 3), ('doc_337', 1), ('doc_342', 1), ('doc_347', 1), ('doc_349', 1), ('doc_350', 1), ('doc_351', 1), ('doc_352', 1), ('doc_354', 1), ('doc_359', 1), ('doc_360', 1), ('doc_364', 1), ('doc_367', 1), ('doc_376', 1), ('doc_377', 1), ('doc_379', 2), ('doc_386', 1), ('doc_388', 1), ('doc_390', 2), ('doc_393', 1), ('doc_394', 1), ('doc_399', 3), ('doc_400', 1), ('doc_402', 1), ('doc_404', 2), ('doc_406', 1), ('doc_407', 1), ('doc_409', 1), ('doc_410', 1), ('doc_411', 1), ('doc_413', 2), ('doc_417', 1), ('doc_420', 1), ('doc_424', 1), ('doc_427', 1), ('doc_435', 2), ('doc_436', 1), ('doc_440', 1), ('doc_443', 1), ('doc_445', 1), ('doc_448', 1), ('doc_449', 1), ('doc_450', 1), ('doc_452', 1), ('doc_453', 1), ('doc_454', 1), ('doc_455', 1), ('doc_456', 2), ('doc_463', 1), ('doc_466', 2), ('doc_470', 2), ('doc_471', 1), ('doc_476', 2), ('doc_478', 1), ('doc_479', 1), ('doc_486', 1), ('doc_487', 2), ('doc_492', 1), ('doc_495', 2), ('doc_496', 1), ('doc_499', 1), ('doc_503', 1), ('doc_504', 1), ('doc_509', 1), ('doc_512', 1), ('doc_513', 2), ('doc_514', 1), ('doc_528', 1), ('doc_538', 1), ('doc_539', 1), ('doc_540', 1), ('doc_543', 1), ('doc_548', 1), ('doc_554', 2), ('doc_555', 1), ('doc_556', 3), ('doc_557', 1), ('doc_565', 1), ('doc_569', 2), ('doc_575', 1), ('doc_576', 1), ('doc_577', 1), ('doc_578', 1), ('doc_579', 2), ('doc_580', 1), ('doc_584', 1), ('doc_586', 1), ('doc_587', 2), ('doc_589', 1), ('doc_593', 1), ('doc_601', 1), ('doc_608', 1), ('doc_611', 1), ('doc_613', 1), ('doc_619', 1), ('doc_620', 1), ('doc_624', 1), ('doc_625', 1), ('doc_629', 1), ('doc_631', 2), ('doc_632', 1), ('doc_638', 1), ('doc_640', 1), ('doc_642', 1), ('doc_643', 1), ('doc_645', 1), ('doc_647', 1), ('doc_648', 8), ('doc_650', 1), ('doc_651', 2), ('doc_653', 1), ('doc_656', 1), ('doc_657', 1), ('doc_662', 1), ('doc_669', 1), ('doc_672', 1), ('doc_682', 1), ('doc_684', 2), ('doc_685', 2), ('doc_688', 1), ('doc_689', 4), ('doc_690', 1), ('doc_697', 1), ('doc_699', 1), ('doc_709', 1), ('doc_713', 1), ('doc_716', 1), ('doc_717', 1), ('doc_718', 2), ('doc_720', 1), ('doc_722', 1), ('doc_724', 1), ('doc_734', 1), ('doc_736', 1), ('doc_739', 1), ('doc_740', 1), ('doc_742', 1), ('doc_746', 2), ('doc_754', 2), ('doc_755', 1), ('doc_758', 1), ('doc_761', 1), ('doc_763', 1), ('doc_768', 1), ('doc_769', 1), ('doc_771', 1), ('doc_772', 2), ('doc_780', 1), ('doc_781', 1), ('doc_792', 1), ('doc_800', 1), ('doc_805', 1), ('doc_806', 1), ('doc_807', 1), ('doc_811', 1), ('doc_812', 2), ('doc_814', 1), ('doc_816', 1), ('doc_824', 2), ('doc_832', 1), ('doc_836', 4), ('doc_837', 1), ('doc_839', 1), ('doc_843', 2), ('doc_849', 1), ('doc_850', 1), ('doc_853', 1), ('doc_857', 1), ('doc_859', 1), ('doc_860', 1), ('doc_861', 1), ('doc_863', 2), ('doc_865', 1), ('doc_867', 1), ('doc_869', 1), ('doc_874', 1), ('doc_877', 1), ('doc_880', 2), ('doc_883', 1), ('doc_884', 1), ('doc_886', 1), ('doc_890', 2), ('doc_893', 2), ('doc_897', 1), ('doc_900', 3), ('doc_901', 2), ('doc_905', 1), ('doc_906', 2), ('doc_907', 1), ('doc_910', 1), ('doc_911', 1), ('doc_913', 1), ('doc_914', 1), ('doc_922', 1), ('doc_926', 1), ('doc_931', 1), ('doc_944', 1), ('doc_947', 1), ('doc_953', 2), ('doc_954', 1), ('doc_957', 1), ('doc_972', 1), ('doc_973', 2), ('doc_983', 1), ('doc_988', 2), ('doc_992', 1), ('doc_993', 1), ('doc_994', 1)]\n",
      "space: 287 documents\n",
      "computers: 9 documents\n",
      "university: 305 documents\n",
      "It took: 0.04664444923400879\n"
     ]
    }
   ],
   "source": [
    "#Now we will work on 1000 docs\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create a dictionary of 1000 documents\n",
    "docs_1000 = {f\"doc_{i+1}\": data[i] for i in range(min(1000, len(data)))}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(doc):\n",
    "    text = doc.get('text', '')  \n",
    "    text = text.lower()  \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  \n",
    "    return text.split()  \n",
    "\n",
    "\n",
    "docs_1000_tokenized = {doc: preprocess_and_tokenize(text) for doc, text in docs_1000.items()}\n",
    "\n",
    "\n",
    "query_words = [\"space\", \"computers\", \"university\"]  \n",
    "\n",
    "start_time_space = time.time()\n",
    "\n",
    "inverted_index_query_words = {word: [] for word in query_words}\n",
    "\n",
    "for doc_name, tokens in docs_1000_tokenized.items():\n",
    "    for word in query_words:\n",
    "        word_count = tokens.count(word)  # Count occurrences of the query word\n",
    "        if word_count > 0:\n",
    "            inverted_index_query_words[word].append((doc_name, word_count))  \n",
    "\n",
    "print(\"Inverted Index for Query Words:\")\n",
    "for word, doc_list in inverted_index_query_words.items():\n",
    "    print(f\"{word}: {doc_list}\")\n",
    "\n",
    "\n",
    "word_doc_counts = {}\n",
    "for word, doc_list in inverted_index_query_words.items():\n",
    "    doc_count = len(doc_list)  # Count the number of documents containing the word\n",
    "    word_doc_counts[word] = doc_count  # Store the count for each word\n",
    "    print(f\"{word}: {doc_count} documents\")\n",
    "\n",
    "\n",
    "end_time_space = time.time()\n",
    "print(\"It took:\", end_time_space - start_time_space)\n",
    "\n",
    "with open(\"index_inverted_1000_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(inverted_index_query_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the inverted index got the same results as boolean matrix but the inverted index found the answer faster,In this case the difference wasn't big but with larger sets of documents it might be a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files of the inverted index\n",
      "\n",
      "Size of 'index_inverted_1000_docs.pkl': 7.51 KB\n",
      "Size of 'index_inverted_100_docs.pkl': 7.51 KB\n",
      "\n",
      "Files of the boolean matrix\n",
      "Size of 'Bool_matrix_1000_docs.pkl': 7.51 KB\n",
      "Size of 'Bool_matrix_100_docs.pkl': 7.51 KB\n"
     ]
    }
   ],
   "source": [
    "#Lets see the sizes of each file.\n",
    "import os\n",
    "\n",
    "# Filepath to your pickled file\n",
    "file_path = \"index_inverted_1000_docs.pkl\"\n",
    "file_path2 = \"index_inverted_100_docs.pkl\"\n",
    "file_path3 = \"Bool_matrix_1000_docs.pkl\"\n",
    "file_path4 = \"Bool_matrix_100_docs.pkl\"\n",
    "\n",
    "# Get the size of the file in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "\n",
    "print(\"Files of the inverted index\\n\")\n",
    "print(f\"Size of '{file_path}': {file_size / 1024:.2f} KB\")  # Size in KB\n",
    "\n",
    "print(f\"Size of '{file_path2}': {file_size / 1024:.2f} KB\\n\")  # Size in KB\n",
    "print(\"Files of the boolean matrix\")\n",
    "\n",
    "print(f\"Size of '{file_path3}': {file_size / 1024:.2f} KB\")  # Size in KB\n",
    "\n",
    "print(f\"Size of '{file_path4}': {file_size / 1024:.2f} KB\")  # Size in KB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
