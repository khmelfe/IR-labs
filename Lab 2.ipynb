{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab 2**\n",
    "Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First We will summarize each of the following Libaries.**\n",
    "Libary one:\n",
    "1) ***NLTK:***\n",
    "This Platform Main goal is to make a bridge between Python programs and human languages data such as documents. The platform works with over 50 corporas(large strucuted sets of texts which are used for NLP works) and lexical resources.\n",
    "It's main abiliites that are relevent for our Dicts : Tokenization,Lemmatization,Stemming,Named Entity Recognition (NER),Synonyms(using wordnet) and WSD.\n",
    "2) ***TextBlob***\n",
    "This Libary is for processing textual data,The idea is to provide a simple API for working on NLPs tasks.\n",
    "It's main abiliites that are relevent for our Dicts :Tokenizations,lemmatizations,Spelling corrections.\n",
    "2) ***spaCy***\n",
    "Open-source libary,designed for producion use and helps to build apps \"understand\" large volumes of text.\n",
    "It's main abiliites that are relevent for our Dicts:Tokenization,Lemmatization,\n",
    "WSD(limited),NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Install all the libaries first.\n",
    "# %pip  install nltk\n",
    "# %pip install numpy\n",
    "# %pip install TextBlob\n",
    "# %pip install spaCy\n",
    "# %pip install pandas\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK is installed. Version: 3.9.1\n",
      "TextBlob is installed.\n",
      "SpaCy is installed. Version: 3.8.3\n"
     ]
    }
   ],
   "source": [
    "#Testing that the libaries are installed\n",
    "try:\n",
    "    import nltk\n",
    "    print(f\"NLTK is installed. Version: {nltk.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"NLTK is not installed.\")\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    print(\"TextBlob is installed.\")\n",
    "except ImportError:\n",
    "    print(\"TextBlob is not installed.\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    print(f\"SpaCy is installed. Version: {spacy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"SpaCy is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will test the libaries and their abillites on arteficual text.(60 words)\n",
    "Art_text = \"Jhon visited the bank to depsoit $500. He noticed the river bnak nearby was bustling with activty. AI is tranforming industres, especially helthcare and finance. NLP tols help anaylze txt. NASA recently annonced a mission to Mars, sparking global excitment. The CEO of Gogle praised inovation during his keynott speach.\"\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Text in tokenize form ['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '$', '500', '.', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'was', 'bustling', 'with', 'activty', '.', 'AI', 'is', 'tranforming', 'industres', ',', 'especially', 'helthcare', 'and', 'finance', '.', 'NLP', 'tols', 'help', 'anaylze', 'txt', '.', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', ',', 'sparking', 'global', 'excitment', '.', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach', '.']\n",
      "The lemmatized words {15: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 16: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 17: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 18: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 19: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 20: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 21: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 22: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 23: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 24: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 25: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 26: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 27: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 28: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 29: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 30: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 31: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 32: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 33: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 34: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 35: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 36: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 37: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 38: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 39: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 40: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 41: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 42: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 43: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 44: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 45: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 46: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 47: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 48: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 49: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 50: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 51: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 52: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 53: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 54: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 55: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 56: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 57: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 58: {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'wa', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}}\n"
     ]
    }
   ],
   "source": [
    "#Lets start With NLTK:\n",
    "#Abillites that it can run:Tokenization,Lemmatization,Stemming,Named Entity \n",
    "# Recognition (NER),Synonyms(using wordnet) and WSD.\n",
    "\n",
    "import nltk as nd\n",
    "import pandas as pd\n",
    "#Tokenize\n",
    "\n",
    "Art_text_tokenize =  nd.word_tokenize(Art_text)\n",
    "print(\"The Text in tokenize form\",Art_text_tokenize)\n",
    "\n",
    "#Lemmatization- will print only the ones that it changed.\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "word_after={}\n",
    "word_before={}\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(Art_text_tokenize):\n",
    "    word_before[i] = Art_text_tokenize[i]\n",
    "    word_after[i] = wnl.lemmatize(Art_text_tokenize[i])\n",
    "    if(word_after != word_before):\n",
    "        words_lemmatized[i] = word_after\n",
    "\n",
    "print(\"The lemmatized words\",words_lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that have been stemmed {0: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 1: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 2: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 3: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 4: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 5: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 6: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 7: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 8: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 9: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 10: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 11: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 12: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 13: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 14: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 15: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 16: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 17: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 18: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 19: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 20: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 21: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 22: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 23: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 24: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 25: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 26: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 27: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 28: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 29: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 30: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 31: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 32: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 33: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 34: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 35: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 36: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 37: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 38: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 39: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 40: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 41: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 42: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 43: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 44: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 45: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 46: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 47: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 48: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 49: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 50: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 51: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 52: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 53: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 54: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 55: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 56: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 57: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}, 58: {0: 'jhon', 1: 'visit', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'he', 10: 'notic', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearbi', 15: 'was', 16: 'bustl', 17: 'with', 18: 'activti', 19: '.', 20: 'ai', 21: 'is', 22: 'tranform', 23: 'industr', 24: ',', 25: 'especi', 26: 'helthcar', 27: 'and', 28: 'financ', 29: '.', 30: 'nlp', 31: 'tol', 32: 'help', 33: 'anaylz', 34: 'txt', 35: '.', 36: 'nasa', 37: 'recent', 38: 'annonc', 39: 'a', 40: 'mission', 41: 'to', 42: 'mar', 43: ',', 44: 'spark', 45: 'global', 46: 'excit', 47: '.', 48: 'the', 49: 'ceo', 50: 'of', 51: 'gogl', 52: 'prais', 53: 'inov', 54: 'dure', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}}\n",
      "Words that have before being stemmed {0: 'Jhon', 1: 'visited', 2: 'the', 3: 'bank', 4: 'to', 5: 'depsoit', 6: '$', 7: '500', 8: '.', 9: 'He', 10: 'noticed', 11: 'the', 12: 'river', 13: 'bnak', 14: 'nearby', 15: 'was', 16: 'bustling', 17: 'with', 18: 'activty', 19: '.', 20: 'AI', 21: 'is', 22: 'tranforming', 23: 'industres', 24: ',', 25: 'especially', 26: 'helthcare', 27: 'and', 28: 'finance', 29: '.', 30: 'NLP', 31: 'tols', 32: 'help', 33: 'anaylze', 34: 'txt', 35: '.', 36: 'NASA', 37: 'recently', 38: 'annonced', 39: 'a', 40: 'mission', 41: 'to', 42: 'Mars', 43: ',', 44: 'sparking', 45: 'global', 46: 'excitment', 47: '.', 48: 'The', 49: 'CEO', 50: 'of', 51: 'Gogle', 52: 'praised', 53: 'inovation', 54: 'during', 55: 'his', 56: 'keynott', 57: 'speach', 58: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "#we will take again the same Art_text_tokenize\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "word_after={}\n",
    "word_before={}\n",
    "word_before_stemed = {}\n",
    "for i,word in enumerate(Art_text_tokenize):\n",
    "    word_before[i] = Art_text_tokenize[i]\n",
    "    word_after[i] = stemmer.stem(Art_text_tokenize[i])\n",
    "    if word_after != word_before:\n",
    "        word_before_stemed[i] = word_before[i]\n",
    "        word_stemed[i] = word_after\n",
    "print(f\"Words that have been stemmed\",word_stemed)\n",
    "print(f\"Words that have before being stemmed\",word_before_stemed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Installations for NER\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entites [('Jhon', 'PERSON'), ('NLP', 'ORGANIZATION'), ('NASA', 'ORGANIZATION'), ('Mars', 'PERSON'), ('CEO of Gogle', 'ORGANIZATION')]\n",
      "Words with at least one synonym and their synsets:\n",
      "ceo: chief_executive_officer.n.01, he: helium.n.01, ai: army_intelligence.n.01, visited: visit.v.01, praised: praise.v.01, finance: finance.n.01, mars: mars.n.01, nearby: nearby.s.01, noticed: detect.v.01, is: be.v.01, especially: particularly.r.01, mission: mission.n.01, was: washington.n.02, bank: bank.n.01, nasa: national_aeronautics_and_space_administration.n.01, sparking: trip.v.04, river: river.n.01, 500: five_hundred.n.01, help: aid.n.02, nlp: natural_language_processing.n.01, bustling: bustle.v.01, recently: recently.r.01, a: angstrom.n.01, global: global.s.01\n",
      "\n",
      "Words with their disambiguated senses and definitions:\n",
      "visited: visit.v.04 - come to see in an official or professional capacity\n",
      "bank: bank.v.07 - cover with ashes so to control the rate of burning\n",
      "500: five_hundred.s.01 - denoting a quantity consisting of 500 items or units\n",
      "He: helium.n.01 - a very light colorless element that is one of the six inert gasses; the most difficult gas to liquefy; occurs in economically extractable amounts in certain natural gases (as those found in Texas and Kansas)\n",
      "noticed: notice.v.04 - express recognition of the presence or existence of, or acquaintance with\n",
      "river: river.n.01 - a large natural stream of water (larger than a creek)\n",
      "nearby: nearby.s.01 - close at hand\n",
      "was: be.v.01 - have the quality of being; (copula, used with an adjective or a predicate noun)\n",
      "bustling: bustling.s.01 - full of energetic and noisy activity\n",
      "AI: three-toed_sloth.n.01 - a sloth that has three long claws on each forefoot and each hindfoot\n",
      "is: exist.v.01 - have an existence, be extant\n",
      "especially: particularly.r.01 - to a distinctly greater extent or degree than is common\n",
      "finance: finance.n.03 - the management of money and credit and banking and investments\n",
      "NLP: natural_language_processing.n.01 - the branch of information science that deals with natural language information\n",
      "help: serve.v.05 - help to some food; help with food or drink\n",
      "NASA: national_aeronautics_and_space_administration.n.01 - an independent agency of the United States government responsible for aviation and spaceflight\n",
      "recently: recently.r.01 - in the recent past\n",
      "a: angstrom.n.01 - a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation\n",
      "mission: mission.n.03 - a special assignment that is given to a person or group\n",
      "Mars: mars.n.01 - a small reddish planet that is the 4th from the sun and is periodically visible to the naked eye; minerals rich in iron cover its surface and are responsible for its characteristic color\n",
      "sparking: trip.v.04 - put in motion or move to act\n",
      "global: ball-shaped.s.01 - having the shape of a sphere or ball; ; ; - Zane Grey\n",
      "CEO: chief_executive_officer.n.01 - the corporate executive responsible for the operations of the firm; reports to a board of directors; may appoint other managers (including a president)\n",
      "praised: praise.v.01 - express approval of\n"
     ]
    }
   ],
   "source": [
    "#NLTK Named Entity \n",
    "# Recognition (NER)\n",
    "#We will use POS_Tags So the NER modal could work better.\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "pos_tags = pos_tag(Art_text_tokenize)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "print(\"Entites\",entites) \n",
    "\n",
    "#Synonyms\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = []\n",
    "\n",
    "for word in Art_text_tokenize:\n",
    "    word = word.lower()\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if wordnet.synsets(word):\n",
    "        if(word != synsets):\n",
    "            words_with_synonyms.append((word, synsets[0].name()))\n",
    "words_with_synonyms = list(set(words_with_synonyms))\n",
    "print(\"Words with at least one synonym and their synsets:\")\n",
    "formatted_output = ', '.join([f\"{word}: {synset}\" for word, synset in words_with_synonyms])\n",
    "print(formatted_output)\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "#we will try to use sentances to get context.\n",
    "sentences = nltk.sent_tokenize(Art_text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "\n",
    "# Display the results\n",
    "print(\"\")\n",
    "print(\"Words with their disambiguated senses and definitions:\")\n",
    "for word, synset_name, definition in word_senses:\n",
    "    print(f\"{word}: {synset_name} - {definition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization : ['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '500', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'was', 'bustling', 'with', 'activty', 'AI', 'is', 'tranforming', 'industres', 'especially', 'helthcare', 'and', 'finance', 'NLP', 'tols', 'help', 'anaylze', 'txt', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', 'sparking', 'global', 'excitment', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach']\n",
      "['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '500', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'wa', 'bustling', 'with', 'activty', 'AI', 'is', 'tranforming', 'industres', 'especially', 'helthcare', 'and', 'finance', 'NLP', 'tols', 'help', 'anaylze', 'txt', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', 'sparking', 'global', 'excitment', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach']\n",
      "word before : {'Jhon'} after: {'Hon'}\n",
      "word before : {'depsoit'} after: {'deposit'}\n",
      "word before : {'bnak'} after: {'bank'}\n",
      "word before : {'activty'} after: {'activity'}\n",
      "word before : {'AI'} after: {'of'}\n",
      "word before : {'tranforming'} after: {'transforming'}\n",
      "word before : {'industres'} after: {'industries'}\n",
      "word before : {'tols'} after: {'told'}\n",
      "word before : {'anaylze'} after: {'analyze'}\n",
      "word before : {'annonced'} after: {'announced'}\n",
      "word before : {'Mars'} after: {'Wars'}\n",
      "word before : {'sparking'} after: {'sparkling'}\n",
      "word before : {'excitment'} after: {'excitement'}\n",
      "word before : {'Gogle'} after: {'Sole'}\n",
      "word before : {'inovation'} after: {'innovation'}\n",
      "word before : {'speach'} after: {'speech'}\n"
     ]
    }
   ],
   "source": [
    "#Lets move to TextBlob\n",
    "#Tokenizations,lemmatizations,Spelling corrections.\n",
    "from textblob import TextBlob \n",
    "#Tokenization\n",
    "blob_obj = TextBlob(Art_text)\n",
    "blob_tokenize = blob_obj.words\n",
    "print(\"Tokenization :\",blob_tokenize)\n",
    "\n",
    "#Lemmatizations\n",
    "blob_lem=[word.lemmatize() for word in blob_tokenize]\n",
    "print(blob_lem)\n",
    "\n",
    "#Spelling Corrections\n",
    "for i,word in enumerate(blob_tokenize):\n",
    "    word_before = blob_tokenize[i]\n",
    "    word_after =blob_tokenize[i].correct()\n",
    "    if word_before != word_after:\n",
    "        print(f\"word before :\",{word_before},\"after:\",{word_after})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art_token ['Jhon', 'visited', 'the', 'bank', 'to', 'depsoit', '$', '500', '.', 'He', 'noticed', 'the', 'river', 'bnak', 'nearby', 'was', 'bustling', 'with', 'activty', '.', 'AI', 'is', 'tranforming', 'industres', ',', 'especially', 'helthcare', 'and', 'finance', '.', 'NLP', 'tols', 'help', 'anaylze', 'txt', '.', 'NASA', 'recently', 'annonced', 'a', 'mission', 'to', 'Mars', ',', 'sparking', 'global', 'excitment', '.', 'The', 'CEO', 'of', 'Gogle', 'praised', 'inovation', 'during', 'his', 'keynott', 'speach', '.']\n",
      "Lemmatization ['Jhon', 'visit', 'the', 'bank', 'to', 'depsoit', '$', '500', '.', 'he', 'notice', 'the', 'river', 'bnak', 'nearby', 'be', 'bustle', 'with', 'activty', '.', 'AI', 'be', 'tranforme', 'industre', ',', 'especially', 'helthcare', 'and', 'finance', '.', 'NLP', 'tol', 'help', 'anaylze', 'txt', '.', 'NASA', 'recently', 'annonce', 'a', 'mission', 'to', 'Mars', ',', 'spark', 'global', 'excitment', '.', 'the', 'ceo', 'of', 'Gogle', 'praise', 'inovation', 'during', 'his', 'keynott', 'speach', '.']\n",
      "Entity: Jhon, Label: PERSON\n",
      "Entity: depsoit, Label: GPE\n",
      "Entity: 500, Label: MONEY\n",
      "Entity: NLP, Label: ORG\n",
      "Entity: NASA, Label: ORG\n",
      "Entity: Mars, Label: LOC\n",
      "Entity: Gogle, Label: ORG\n",
      "Entity: keynott speach, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "#SPACY\n",
    "# Tokenization,Lemmatization, WSD(limited),NER.\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "nlp = English()#english pipline\n",
    "\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(Art_text)\n",
    "Art_token = [token.text for token in tokens]\n",
    "print(\"Art_token\",Art_token)\n",
    "\n",
    "#Lemmatization\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(Art_text) #we need a raw version.\n",
    "print(\"Lemmatization\",[token.lemma_ for token in doc])\n",
    "\n",
    "#NER\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**part 4**\n",
    "Working on our choosen docs for lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\filex\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') #punctuations\n",
    "nltk.download('stopwords')#Stopwords.\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "cats = ['comp.windows.x', 'sci.space'] #getting two options\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize #Libray to tokenize the words.\n",
    "from nltk.corpus import stopwords\n",
    "import string  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  #Libary from Counters.\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=cats)\n",
    "\n",
    "\n",
    "#lets do some preprocessing\n",
    "#First lets combine them into one big category.\n",
    "category_texts_unified = [\n",
    "    {\"text\": doc, \"category\": newsgroups.target_names[target]}\n",
    "    for doc, target in zip(newsgroups.data, newsgroups.target)\n",
    "    if target in [newsgroups.target_names.index(cat) for cat in cats]\n",
    "]\n",
    "stops = set(stopwords.words('english')) # stopword from english.\n",
    "\n",
    "#Removing Lowercases,non-alphabetical characters\n",
    "#Tokenizeing ,removing stopwords and punctuation\n",
    "Clean_text = []\n",
    "for doc in category_texts_unified:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', doc[\"text\"].lower())\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [w for w in words if w not in stops and w not in string.punctuation]\n",
    "    # Save cleaned text with category\n",
    "    Clean_text.append({\"text\": ' '.join(filtered_words), \"category\": doc[\"category\"]})\n",
    "\n",
    "\n",
    "\n",
    "#Let's look for mispalling.\n",
    "mispalled_words = []\n",
    "for entry in Clean_text:\n",
    "    # Tokenize the cleaned text\n",
    "    words = word_tokenize(entry[\"text\"])\n",
    "    for word in words:\n",
    "        blob = TextBlob(word)\n",
    "        corrected_word = str(blob.correct())\n",
    "        if corrected_word != word:\n",
    "            mispalled_words.append((word, corrected_word))\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # Save preprocessed data to a Pickle file\n",
    "# with open(\"preprocessed_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(Clean_text, f)\n",
    "\n",
    "# print(\"Data saved to preprocessed_data.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after preprocssing, We will start doing the abilities we mentioned in the begining.\n",
    "We can conclude that fixing mispalling takes the most time and effort, therefore as part of our attempts to find the best algorthem for both accurcy and time efficency.\n",
    "We will judge the outputs on how many ACRONYM,SYNONYMS,WSD,NER;we got on each test.\n",
    "#For the first test, We will do :\n",
    "1)lemmatization-nltk \n",
    "2)stemming -nltk\n",
    "3)Synonyms -nltk using wordnet\n",
    "4)WSD-nltk\n",
    "5)NER-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 1.8750958442687988\n",
      "lemmatization time: 1.8051435947418213\n",
      "Stemming time: 4.695420265197754\n",
      "Synonyms time: 7.5493669509887695\n",
      "WSD time: 9809.51560473442\n",
      "NER time: 91.48276424407959\n",
      "The Results are in!\n",
      "The amount of lemmatized words: 33513\n",
      "The amount of stemmed words: 134885\n",
      "The amount of words with at least two synonyms: 10405\n",
      "The amount of NER entities found: 3\n",
      "The amount of words with sense found (WSD): 257634\n",
      "Overall time it took: 9916.932371377945\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import nltk as nd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    " \n",
    "start_time_overall = time.time() #Starting the timer.\n",
    "#Tokenization\n",
    "start_time_task = time.time()\n",
    "data_tokenized = []\n",
    "for entry in data:\n",
    "    tokenized_data = nd.word_tokenize(entry[\"text\"])\n",
    "    data_tokenized.extend(tokenized_data)  # Flatten tokens into a single list\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Tokenization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#lemmatization\n",
    "start_time_task = time.time()\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    word_afte = wnl.lemmatize(word)\n",
    "    if(word_afte != word):\n",
    "        words_lemmatized[i] = (word,word_afte)\n",
    "end_time_task = time.time()\n",
    "print(\"lemmatization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#stemming\n",
    "start_time_task = time.time()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    stemed_word = stemmer.stem(word)\n",
    "    if stemed_word != word:\n",
    "        word_stemed[i] = (word,stemed_word)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Stemming time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "#Synonyms-we will count the amout of words that have at least two synonyms\n",
    "start_time_task = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = {}\n",
    "\n",
    "for word in data_tokenized:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets)>=2: #checks if there is at least two synonmys per word.\n",
    "        synonyms = {lemma.name() for syn in synsets for lemma in syn.lemmas()}\n",
    "        if len(synonyms) >= 2:  # Check if there are at least two distinct synonyms\n",
    "            words_with_synonyms[word] = list(synonyms)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Synonyms time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "#we will try to use sentances to get context.\n",
    "start_time_task = time.time()\n",
    "\n",
    "text_string = \" \".join(data_tokenized)\n",
    "sentences = nd.sent_tokenize(text_string)\n",
    "tokenized_sentences = [nd.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "end_time_task = time.time()\n",
    "print(\"WSD time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "\n",
    "#NER\n",
    "start_time_task = time.time()\n",
    "pos_tags = pos_tag(data_tokenized)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"NER time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"The Results are in!\")\n",
    "print(f'The amount of lemmatized words: {len(words_lemmatized)}')\n",
    "print(f'The amount of stemmed words: {len(word_stemed)}')\n",
    "print(f'The amount of words with at least two synonyms: {len(words_with_synonyms)}')\n",
    "print(f'The amount of NER entities found: {len(entites)}')\n",
    "print(f'The amount of words with sense found (WSD): {len(word_senses)}')\n",
    "print(\"Overall time it took:\",exc_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 1.815438985824585\n",
      "lemmatization time: 5.336212396621704\n",
      "Synonyms time: 10.8156259059906\n",
      "Stemming time: 5.025677919387817\n",
      "NER time: 94.18226456642151\n",
      "WSD time: 8215.4396276474\n",
      "The Results are in!\n",
      "The amount of lemmatized words: 33513\n",
      "The amount of stemmed words: 134885\n",
      "The amount of words with at least two synonyms: 10405\n",
      "The amount of NER entities found: 3\n",
      "The amount of words with sense found (WSD): 257634\n",
      "Overall time it took: 8332.61590385437\n"
     ]
    }
   ],
   "source": [
    "#lets try diffrenet sequence of abiliities.\n",
    "#1)lemmatization-nltk \n",
    "#2)Synonyms -nltk using wordnet\n",
    "#3)stemming -nltk\n",
    "#4)NER-nltk\n",
    "#5)WSD-nltk\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import nltk as nd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "start_time_overall = time.time() #Starting the timer.\n",
    "#Tokenization\n",
    "start_time_task = time.time()\n",
    "data_tokenized = []\n",
    "for entry in data:\n",
    "    tokenized_data = nd.word_tokenize(entry[\"text\"])\n",
    "    data_tokenized.extend(tokenized_data)  # Flatten tokens into a single list\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Tokenization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#lemmatization\n",
    "start_time_task = time.time()\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    word_afte = wnl.lemmatize(word)\n",
    "    if(word_afte != word):\n",
    "        words_lemmatized[i] = (word,word_afte)\n",
    "end_time_task = time.time()\n",
    "print(\"lemmatization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#Synonyms-we will count the amout of words that have at least two synonyms\n",
    "start_time_task = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = {}\n",
    "\n",
    "for word in data_tokenized:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets)>=2: #checks if there is at least two synonmys per word.\n",
    "        synonyms = {lemma.name() for syn in synsets for lemma in syn.lemmas()}\n",
    "        if len(synonyms) >= 2:  # Check if there are at least two distinct synonyms\n",
    "            words_with_synonyms[word] = list(synonyms)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Synonyms time:\",end_time_task-start_time_task)\n",
    "\n",
    "#stemming\n",
    "start_time_task = time.time()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    stemed_word = stemmer.stem(word)\n",
    "    if stemed_word != word:\n",
    "        word_stemed[i] = (word,stemed_word)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Stemming time:\",end_time_task-start_time_task)\n",
    "\n",
    "\n",
    "#NER\n",
    "start_time_task = time.time()\n",
    "pos_tags = pos_tag(data_tokenized)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"NER time:\",end_time_task-start_time_task)\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "#we will try to use sentances to get context.\n",
    "start_time_task = time.time()\n",
    "\n",
    "text_string = \" \".join(data_tokenized)\n",
    "sentences = nd.sent_tokenize(text_string)\n",
    "tokenized_sentences = [nd.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "end_time_task = time.time()\n",
    "print(\"WSD time:\",end_time_task-start_time_task)\n",
    "\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"The Results are in!\")\n",
    "print(f'The amount of lemmatized words: {len(words_lemmatized)}')\n",
    "print(f'The amount of stemmed words: {len(word_stemed)}')\n",
    "print(f'The amount of words with at least two synonyms: {len(words_with_synonyms)}')\n",
    "print(f'The amount of NER entities found: {len(entites)}')\n",
    "print(f'The amount of words with sense found (WSD): {len(word_senses)}')\n",
    "print(\"Overall time it took:\",exc_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Test \n",
    "#1)WSD-nltk\n",
    "#2)NER-nltk\n",
    "#3)lemmatization-nltk \n",
    "#4)Synonyms -nltk using wordnet\n",
    "#5)stemming -nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 1.7612321376800537\n",
      "WSD time: 7260.057300329208\n",
      "NER time: 84.98901629447937\n",
      "lemmatization time: 1.7268247604370117\n",
      "Synonyms time: 7.27872109413147\n",
      "Stemming time: 4.529211521148682\n",
      "The Results are in!\n",
      "The amount of lemmatized words: 33513\n",
      "The amount of stemmed words: 134885\n",
      "The amount of words with at least two synonyms: 10405\n",
      "The amount of NER entities found: 3\n",
      "The amount of words with sense found (WSD): 257634\n",
      "Overall time it took: 7360.344216585159\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import nltk as nd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "start_time_overall = time.time() #Starting the timer.\n",
    "#Tokenization\n",
    "start_time_task = time.time()\n",
    "data_tokenized = []\n",
    "for entry in data:\n",
    "    tokenized_data = nd.word_tokenize(entry[\"text\"])\n",
    "    data_tokenized.extend(tokenized_data)  # Flatten tokens into a single list\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Tokenization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "#we will try to use sentances to get context.\n",
    "start_time_task = time.time()\n",
    "\n",
    "text_string = \" \".join(data_tokenized)\n",
    "sentences = nd.sent_tokenize(text_string)\n",
    "tokenized_sentences = [nd.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_senses = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        sense = lesk(sentence, word)  \n",
    "        if sense: \n",
    "            word_senses.append((word, sense.name(), sense.definition()))\n",
    "end_time_task = time.time()\n",
    "print(\"WSD time:\",end_time_task-start_time_task)\n",
    "\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "#NER\n",
    "start_time_task = time.time()\n",
    "pos_tags = pos_tag(data_tokenized)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "entites = []\n",
    "\n",
    "for text in ner_tree:\n",
    "    if hasattr(text,'label'):\n",
    "        entity_name = \" \".join([leaf[0] for leaf in text])\n",
    "        entity_type = text.label()\n",
    "        entites.append((entity_name, entity_type))\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"NER time:\",end_time_task-start_time_task)\n",
    "\n",
    "#lemmatization\n",
    "start_time_task = time.time()\n",
    "wnl = nd.WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    word_afte = wnl.lemmatize(word)\n",
    "    if(word_afte != word):\n",
    "        words_lemmatized[i] = (word,word_afte)\n",
    "end_time_task = time.time()\n",
    "print(\"lemmatization time:\",end_time_task-start_time_task)\n",
    "\n",
    "#Synonyms-we will count the amout of words that have at least two synonyms\n",
    "start_time_task = time.time()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "words_with_synonyms = {}\n",
    "\n",
    "for word in data_tokenized:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets)>=2: #checks if there is at least two synonmys per word.\n",
    "        synonyms = {lemma.name() for syn in synsets for lemma in syn.lemmas()}\n",
    "        if len(synonyms) >= 2:  # Check if there are at least two distinct synonyms\n",
    "            words_with_synonyms[word] = list(synonyms)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Synonyms time:\",end_time_task-start_time_task)\n",
    "\n",
    "#stemming\n",
    "start_time_task = time.time()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "word_stemed = {}\n",
    "\n",
    "for i,word in enumerate(data_tokenized):\n",
    "    stemed_word = stemmer.stem(word)\n",
    "    if stemed_word != word:\n",
    "        word_stemed[i] = (word,stemed_word)\n",
    "\n",
    "end_time_task = time.time()\n",
    "print(\"Stemming time:\",end_time_task-start_time_task)\n",
    "end_time = time.time()\n",
    "exc_time = end_time-start_time_overall\n",
    "\n",
    "# Display the results\n",
    "print(\"The Results are in!\")\n",
    "print(f'The amount of lemmatized words: {len(words_lemmatized)}')\n",
    "print(f'The amount of stemmed words: {len(word_stemed)}')\n",
    "print(f'The amount of words with at least two synonyms: {len(words_with_synonyms)}')\n",
    "print(f'The amount of NER entities found: {len(entites)}')\n",
    "print(f'The amount of words with sense found (WSD): {len(word_senses)}')\n",
    "print(\"Overall time it took:\",exc_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that The best organzation of the abiliites is the last test , and all the other tests preformed the same, but the last test was the fastest.\n",
    "\n",
    "**lab 2 part 2**\n",
    "In this part we will first examine 100 docs, and then for 1000 docs.\n",
    "We will build in the end ,Matrix boolean and reversed indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start with taking 100 docs from our preprossced data.\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "docs_100 ={}\n",
    "docs_100 = {f\"doc_{i+1}\": data[i] for i in range(min(100, len(data)))}\n",
    "\n",
    "#Lets built a Boolean Matrix\n",
    "#we will build three different query tjat search these three words:\n",
    "#1) space 2)computers 3 ) university\n",
    "\n",
    "#1 test\n",
    "start_time_space = time.time()\n",
    "query_word = \"space\"\n",
    "\n",
    "bool_matrix = np.array([int(query_word in word.lower()) for doc in docs_100])\n",
    "print(bool_matrix)\n",
    "\n",
    "end_time_space = time.time()\n",
    "print(\"It took :\",end_time_space-start_time_space)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
